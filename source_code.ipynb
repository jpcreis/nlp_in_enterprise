{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "\n",
    "Tokenization is the process of splitting text into minimal meaningful units such as words, punctuation marks, symbols, etc. For example, the sentence \"We live in Paris\" could be tokenized into four tokens: We, live, in, Paris. Tokenization is typically the first step of every NLP process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of Tokens:  4\n",
      "The tokens: \n",
      "We\n",
      "live\n",
      "in\n",
      "Paris\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'question_tokens'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-cce6edd9cadc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;31m# View first question\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[0mexample_question\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mquestion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m \u001b[0mexample_question_tokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mquestion_tokens\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"The first questio is:\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexample_question\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   5139\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5140\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5141\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5142\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5143\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'question_tokens'"
     ]
    }
   ],
   "source": [
    "# Small example\n",
    "sentence = nlp.tokenizer(\"We live in Paris\")\n",
    "print(\"The number of Tokens: \" , len(sentence))\n",
    "\n",
    "print(\"The tokens: \")\n",
    "for words in sentence:\n",
    "    print(words)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################\n",
    "    \n",
    "import pandas as pd\n",
    "import os\n",
    "cwd = os.getcwd()\n",
    "\n",
    "# Import question\n",
    "data = pd.read_csv(cwd+'\\JEOPARDY_CSV_reduced.csv')\n",
    "data = pd.DataFrame(data=data)\n",
    "\n",
    "# lowercase, strip whitespace, and view column names\n",
    "data.columns = map(lambda x: x.lower().strip(), data.columns)\n",
    "\n",
    "# Reduce size of data\n",
    "data = data[0:1000]\n",
    "\n",
    "# Tokenize Jeopardy Questions\n",
    "data[\"question_tokens\"] = data[\"question\"].apply(lambda x: nlp(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first questio is:\n",
      "For the last 8 years of his life, Galileo was under house arrest for espousing this man's theory\n",
      "The tokens from the first question are:\n",
      "For\n",
      "the\n",
      "last\n",
      "8\n",
      "years\n",
      "of\n",
      "his\n",
      "life\n",
      ",\n",
      "Galileo\n",
      "was\n",
      "under\n",
      "house\n",
      "arrest\n",
      "for\n",
      "espousing\n",
      "this\n",
      "man\n",
      "'s\n",
      "theory\n"
     ]
    }
   ],
   "source": [
    "# View first question\n",
    "example_question = data.question[0]\n",
    "example_question_tokens = data.question_tokens[0]\n",
    "print(\"The first questio is:\")\n",
    "print(example_question)\n",
    "\n",
    "\n",
    "# Individual tokens of first question\n",
    "print(\"The tokens from the first question are:\")\n",
    "for tokens in example_question_tokens:\n",
    "    print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part-of-Speech (POS) tagging\n",
    "\n",
    "Part-of-speech (POS) tagging is the process of assigning word types to tokens, such as noun, pronoun, verb, adverb, adjective, conjuntion, preposition, interjection, etc. For \"We live in Paris\", the parts of speech are: pronoun, verb, preposition, and noun. This part-of-speech gives each token a bit more metadata, making it easier for the machine to assign relationships between each token and every other token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are the Part-of-Speech tags for each token in the first questions: \n",
      "For ADP adposition\n",
      "the DET determiner\n",
      "last ADJ adjective\n",
      "8 NUM numeral\n",
      "years NOUN noun\n",
      "of ADP adposition\n",
      "his PRON pronoun\n",
      "life NOUN noun\n",
      ", PUNCT punctuation\n",
      "Galileo PROPN proper noun\n",
      "was AUX auxiliary\n",
      "under ADP adposition\n",
      "house NOUN noun\n",
      "arrest NOUN noun\n",
      "for ADP adposition\n",
      "espousing VERB verb\n",
      "this DET determiner\n",
      "man NOUN noun\n",
      "'s PART particle\n",
      "theory NOUN noun\n"
     ]
    }
   ],
   "source": [
    "# Part of speech tags for tokens in the first question\n",
    "\n",
    "print(\"Here are the Part-of-Speech tags for each token in the first questions: \")\n",
    "for token in example_question_tokens:\n",
    "    print(token.text, token.pos_, spacy.explain(token.pos_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependency Parsing\n",
    "\n",
    "Dependency parsing involves labeling the relationships between individual tokens, assigning a syntactic structure to the sentence. Once the relationships are labeled, the entire sentence can be structured as a series of relationships among sets of tokens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For prep prepositional modifier\n",
      "the det determiner\n",
      "last amod adjectival modifier\n",
      "8 nummod numeric modifier\n",
      "years pobj object of preposition\n",
      "of prep prepositional modifier\n",
      "his poss possession modifier\n",
      "life pobj object of preposition\n",
      ", punct punctuation\n",
      "Galileo nsubj nominal subject\n",
      "was ROOT None\n",
      "under prep prepositional modifier\n",
      "house compound compound\n",
      "arrest pobj object of preposition\n",
      "for prep prepositional modifier\n",
      "espousing pcomp complement of preposition\n",
      "this det determiner\n",
      "man poss possession modifier\n",
      "'s case case marking\n",
      "theory dobj direct object\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"b753596311fc4364b0fd687770470b65-0\" class=\"displacy\" width=\"1950\" height=\"387.0\" direction=\"ltr\" style=\"max-width: none; height: 387.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"297.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">For</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"297.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"150\">the</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"150\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"297.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"250\">last</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"250\">ADJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"297.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"350\">8</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"350\">NUM</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"297.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"450\">years</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"450\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"297.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"550\">of</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"550\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"297.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"650\">his</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"650\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"297.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">life,</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"297.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"850\">Galileo</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"850\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"297.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"950\">was</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"950\">AUX</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"297.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1050\">under</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1050\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"297.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1150\">house</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1150\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"297.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1250\">arrest</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1250\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"297.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1350\">for</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1350\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"297.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1450\">espousing</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1450\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"297.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1550\">this</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1550\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"297.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1650\">man</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1650\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"297.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1750\">'s</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1750\">PART</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"297.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1850\">theory</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1850\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-b753596311fc4364b0fd687770470b65-0-0\" stroke-width=\"2px\" d=\"M70,252.0 C70,2.0 950.0,2.0 950.0,252.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-b753596311fc4364b0fd687770470b65-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,254.0 L62,242.0 78,242.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-b753596311fc4364b0fd687770470b65-0-1\" stroke-width=\"2px\" d=\"M170,252.0 C170,102.0 440.0,102.0 440.0,252.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-b753596311fc4364b0fd687770470b65-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M170,254.0 L162,242.0 178,242.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-b753596311fc4364b0fd687770470b65-0-2\" stroke-width=\"2px\" d=\"M270,252.0 C270,152.0 435.0,152.0 435.0,252.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-b753596311fc4364b0fd687770470b65-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M270,254.0 L262,242.0 278,242.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-b753596311fc4364b0fd687770470b65-0-3\" stroke-width=\"2px\" d=\"M370,252.0 C370,202.0 430.0,202.0 430.0,252.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-b753596311fc4364b0fd687770470b65-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nummod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M370,254.0 L362,242.0 378,242.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-b753596311fc4364b0fd687770470b65-0-4\" stroke-width=\"2px\" d=\"M70,252.0 C70,52.0 445.0,52.0 445.0,252.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-b753596311fc4364b0fd687770470b65-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M445.0,254.0 L453.0,242.0 437.0,242.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-b753596311fc4364b0fd687770470b65-0-5\" stroke-width=\"2px\" d=\"M470,252.0 C470,202.0 530.0,202.0 530.0,252.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-b753596311fc4364b0fd687770470b65-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M530.0,254.0 L538.0,242.0 522.0,242.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-b753596311fc4364b0fd687770470b65-0-6\" stroke-width=\"2px\" d=\"M670,252.0 C670,202.0 730.0,202.0 730.0,252.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-b753596311fc4364b0fd687770470b65-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">poss</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M670,254.0 L662,242.0 678,242.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-b753596311fc4364b0fd687770470b65-0-7\" stroke-width=\"2px\" d=\"M570,252.0 C570,152.0 735.0,152.0 735.0,252.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-b753596311fc4364b0fd687770470b65-0-7\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M735.0,254.0 L743.0,242.0 727.0,242.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-b753596311fc4364b0fd687770470b65-0-8\" stroke-width=\"2px\" d=\"M870,252.0 C870,202.0 930.0,202.0 930.0,252.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-b753596311fc4364b0fd687770470b65-0-8\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M870,254.0 L862,242.0 878,242.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-b753596311fc4364b0fd687770470b65-0-9\" stroke-width=\"2px\" d=\"M970,252.0 C970,202.0 1030.0,202.0 1030.0,252.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-b753596311fc4364b0fd687770470b65-0-9\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1030.0,254.0 L1038.0,242.0 1022.0,242.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-b753596311fc4364b0fd687770470b65-0-10\" stroke-width=\"2px\" d=\"M1170,252.0 C1170,202.0 1230.0,202.0 1230.0,252.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-b753596311fc4364b0fd687770470b65-0-10\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1170,254.0 L1162,242.0 1178,242.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-b753596311fc4364b0fd687770470b65-0-11\" stroke-width=\"2px\" d=\"M1070,252.0 C1070,152.0 1235.0,152.0 1235.0,252.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-b753596311fc4364b0fd687770470b65-0-11\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1235.0,254.0 L1243.0,242.0 1227.0,242.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-b753596311fc4364b0fd687770470b65-0-12\" stroke-width=\"2px\" d=\"M970,252.0 C970,52.0 1345.0,52.0 1345.0,252.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-b753596311fc4364b0fd687770470b65-0-12\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1345.0,254.0 L1353.0,242.0 1337.0,242.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-b753596311fc4364b0fd687770470b65-0-13\" stroke-width=\"2px\" d=\"M1370,252.0 C1370,202.0 1430.0,202.0 1430.0,252.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-b753596311fc4364b0fd687770470b65-0-13\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pcomp</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1430.0,254.0 L1438.0,242.0 1422.0,242.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-b753596311fc4364b0fd687770470b65-0-14\" stroke-width=\"2px\" d=\"M1570,252.0 C1570,202.0 1630.0,202.0 1630.0,252.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-b753596311fc4364b0fd687770470b65-0-14\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1570,254.0 L1562,242.0 1578,242.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-b753596311fc4364b0fd687770470b65-0-15\" stroke-width=\"2px\" d=\"M1670,252.0 C1670,152.0 1835.0,152.0 1835.0,252.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-b753596311fc4364b0fd687770470b65-0-15\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">poss</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1670,254.0 L1662,242.0 1678,242.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-b753596311fc4364b0fd687770470b65-0-16\" stroke-width=\"2px\" d=\"M1670,252.0 C1670,202.0 1730.0,202.0 1730.0,252.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-b753596311fc4364b0fd687770470b65-0-16\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">case</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1730.0,254.0 L1738.0,242.0 1722.0,242.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-b753596311fc4364b0fd687770470b65-0-17\" stroke-width=\"2px\" d=\"M1470,252.0 C1470,52.0 1845.0,52.0 1845.0,252.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-b753596311fc4364b0fd687770470b65-0-17\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1845.0,254.0 L1853.0,242.0 1837.0,242.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Dependency Parsing tags for tokens in the first question\n",
    "\n",
    "for token in example_question_tokens:\n",
    "    print(token.text, token.dep_, spacy.explain(token.dep_))\n",
    "    \n",
    "# Visualize the dependency parse\n",
    "from spacy import displacy\n",
    "\n",
    "displacy.render(example_question_tokens, style='dep', jupyter=True, options={'distance': 100})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chunking\n",
    "\n",
    "Chunking involves combining related tokens into a signle token, creating related noun groups, related verb groups, etc. For example, \"New York City\" could be treated as a single token / chunk instead of as three separate tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Only tokens:\n",
      "My\n",
      "parents\n",
      "live\n",
      "in\n",
      "New\n",
      "York\n",
      "City\n",
      "\n",
      "Chunks:\n",
      "My parents\n",
      "New York City\n"
     ]
    }
   ],
   "source": [
    "# Tokens without chunking\n",
    "\n",
    "print(\"Only tokens:\")\n",
    "sentence = \"My parents live in New York City\"\n",
    "for token in nlp(sentence):\n",
    "    print(token.text)\n",
    "    \n",
    "print(\"\\nChunks:\")\n",
    "for chunk in nlp(sentence).noun_chunks:\n",
    "    print(chunk.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatization\n",
    "\n",
    "Lemmatization is the process of converting words into their base forms. For example, lemmatization converts \"horses\" to \"horse\", \"slept\" to \"sleep\", and \"biggest\" to \"big\". It allows the machine to simplify the text processing work it has to perform.\n",
    "\n",
    "# Stemming\n",
    "\n",
    "Stemming is a process related to lemmatization, but simpler. Stemming reduces words to their word stems. Stemming algorithms are typically rule-based. For example, the word \"biggest\" would be reduced to \"big\", but the word \"slept\" would not be reduced at all. Stemming sometimes results in nonsensical subwords, and we prefer lemmatization to stemming for this reason."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     original lemmatized\n",
      "0         For        for\n",
      "1         the        the\n",
      "2        last       last\n",
      "3           8          8\n",
      "4       years       year\n",
      "5          of         of\n",
      "6         his        his\n",
      "7        life       life\n",
      "8           ,          ,\n",
      "9     Galileo    Galileo\n",
      "10        was         be\n",
      "11      under      under\n",
      "12      house      house\n",
      "13     arrest     arrest\n",
      "14        for        for\n",
      "15  espousing    espouse\n",
      "16       this       this\n",
      "17        man        man\n",
      "18         's         's\n",
      "19     theory     theory\n"
     ]
    }
   ],
   "source": [
    "# Print Lemmatization for tokens in the first question\n",
    "\n",
    "lemmatization = pd.DataFrame(data=[], columns=[\"original\", \"lemmatized\"])\n",
    "i = 0\n",
    "for token in example_question_tokens:\n",
    "    lemmatization.loc[i, \"original\"] = token.text\n",
    "    lemmatization.loc[i, \"lemmatized\"] = token.lemma_\n",
    "    i = i+1\n",
    "    \n",
    "print(lemmatization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named Entity Recognition (NER)\n",
    "\n",
    "Named Entity Recognition (NER), is the process of assigning labels to known objects (or entities) such as person, organization, location, date, currency, etc. In \"We live in Paris\", \"Paris\" would be marked as the location. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "George Washington was an American political leader, military general, statesman, and Founding Father of the United States, who served as the first president of the United States from 1789 to 1797.\n",
      "\n",
      "Text Start End Label\n",
      "George Washington 0 17 PERSON\n",
      "American 25 33 NORP\n",
      "the United States 104 121 GPE\n",
      "first 141 146 ORDINAL\n",
      "the United States 160 177 GPE\n",
      "1789 to 1797 183 195 DATE\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    George Washington\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " was an \n",
       "<mark class=\"entity\" style=\"background: #c887fb; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    American\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">NORP</span>\n",
       "</mark>\n",
       " political leader, military general, statesman, and Founding Father of \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    the United States\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       ", who served as the \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    first\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORDINAL</span>\n",
       "</mark>\n",
       " president of \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    the United States\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " from \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    1789 to 1797\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       ".</br></div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Print NER results\n",
    "example_sentence = \"George Washington was an American political leader, military general, statesman, and Founding Father of the United States, who served as the first president of the United States from 1789 to 1797.\\n\"\n",
    "\n",
    "print(example_sentence)\n",
    "\n",
    "print(\"Text Start End Label\")\n",
    "doc = nlp(example_sentence)\n",
    "for token in doc.ents:\n",
    "    print(token.text, token.start_char, token.end_char, token.label_)\n",
    "    \n",
    "    \n",
    "displacy.render(doc, style=\"ent\", jupyter=True, options={'distance': 100})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named Entity Linking (NEL)\n",
    "\n",
    "Entity linking is the process of disambiguanting entitie to an external database, linking text in on form or another. This is important both for entity resolution applications (e.g., deduping datasets) and information retrieval applications. In the George W. Bush example, we would eant to resolve all instances of \"George W. Bush\" as \"George W. Bush\", but not to \"George H. W. Bush\", George W. Bush's father and also former US President."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
